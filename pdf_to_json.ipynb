{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Convert files from database into readable data\n",
    "The following 2 cells access the PDF stored in the 'docs' directory (essentially our knowledge base, in this case), extract text from it, and convert it into JSON format indexed by page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    page_texts = []\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text = page.get_text()\n",
    "        page_texts.append(text)\n",
    "    return page_texts\n",
    "\n",
    "pdf=Path(\"docs/QGenda Whitepaper.pdf\")\n",
    "\n",
    "raw_text = extract_text_from_pdf(pdf)\n",
    "# print(raw_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pagination\n",
    "import json\n",
    "import re\n",
    "from utilities import clean_text\n",
    "\n",
    "# Regular expression pattern to match copyright text\n",
    "pattern = r\"Copyright Â© 2024 QGenda, LLC All rights reserved.\"\n",
    "\n",
    "# Split data after each occurrence of the pattern\n",
    "pages = []\n",
    "current_page = []\n",
    "for index, item in enumerate(raw_text):\n",
    "    current_page.append(item)\n",
    "    if re.search(pattern, item):\n",
    "        cleaned_text = clean_text(\"\".join(current_page))\n",
    "        pages.append({\"index\": len(pages), \"text\": cleaned_text})\n",
    "        current_page = []\n",
    "\n",
    "# Convert the pages list to JSON format\n",
    "pages_json = json.dumps(pages, indent=4)\n",
    "\n",
    "# Output the result\n",
    "#print(pages_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Lazy way: Use LLM to clean up outputs.\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain.docstore.document import Document\\nfrom langchain.chains import LLMChain\\nfrom utilities import get_openai_key\\n\\napi_key = get_openai_key()\\n\\nllm = ChatOpenAI(model_name=\\'gpt-4o-mini\\', temperature=1.0)\\ncleaned_pages = []\\nfor page in pages:\\n    raw_text = page[\\'text\\']\\n    # Create the chain using the RunnableSequence\\n    chain_qa = prompt | llm\\n    llm_results = chain_qa.invoke({\"summaries\": raw_text}, return_only_outputs=True)\\n    cleaned_text = llm_results.content\\n    cleaned_pages.append({\"index\": page[\\'index\\'], \"text\": cleaned_text})\\n\\n# Convert the cleaned pages list to JSON format\\ncleaned_pages_json = json.dumps(cleaned_pages, indent=4)\\n\\n# Output the result\\nprint(cleaned_pages_json)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Lazy way: Use LLM to clean up outputs.\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains import LLMChain\n",
    "from utilities import get_openai_key\n",
    "\n",
    "api_key = get_openai_key()\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=1.0)\n",
    "cleaned_pages = []\n",
    "for page in pages:\n",
    "    raw_text = page['text']\n",
    "    # Create the chain using the RunnableSequence\n",
    "    chain_qa = prompt | llm\n",
    "    llm_results = chain_qa.invoke({\"summaries\": raw_text}, return_only_outputs=True)\n",
    "    cleaned_text = llm_results.content\n",
    "    cleaned_pages.append({\"index\": page['index'], \"text\": cleaned_text})\n",
    "\n",
    "# Convert the cleaned pages list to JSON format\n",
    "cleaned_pages_json = json.dumps(cleaned_pages, indent=4)\n",
    "\n",
    "# Output the result\n",
    "print(cleaned_pages_json)'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a .json file of the cleaned data for later use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "#original_f = Path(pdf)\n",
    "new_f = pdf.with_stem(pdf.stem + '_cleaned')\n",
    "json_format = new_f.with_suffix('.json')\n",
    "\n",
    "with open(json_format, 'w', encoding='utf-8') as file:\n",
    "    file.write(pages_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
